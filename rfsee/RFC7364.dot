digraph Flow {
  layout=twopi;
  root=RFC7364;
  overlap=false;
  RFC7364 [label="RFC7364
Problem Statement: Overlays for Network
Virtualization
2014", shape=ellipse, fillcolor="#e8f0ff", tooltip="This document describes issues associated with providing multi-tenancy in large data center networks and how these issues may be addressed using an overlay-based network virtualization approach.  A key multi-tenancy requirement is traffic isolation so that one tenant's traffic is not visible to any other tenant.  Another requirement is address space isolation so that different tenants can use the same address space within different virtual networks.  Traffic and address space isolation is achieved by assigning one or more virtual networks to each tenant, where traffic within a virtual network can only cross into another virtual network in a controlled fashion (e.g., via a configured router and/or a security gateway).  Additional functionality is required to provision virtual networks, associating a virtual machine's network interface(s) with the appropriate virtual network and maintaining that association as the virtual machine is activated, migrated, and/or deactivated.  Use of an overlay-based approach enables scalable deployment on large network infrastructures.", style=filled, fillcolor=green, fontsize=18, penwidth=3, fontweight=bold]; 

  obs_by [label="obsoletes", shape=box, style=filled, fillcolor=lightblue];
  obs_by -> RFC7364;

  obs [label="obsoletes", shape=box, style=filled, fillcolor=lightblue];
  RFC7364 -> obs;

  updates [label="updates", shape=box, style=filled, fillcolor=lightyellow];
  RFC7364 -> updates;

  updated_by [label="updates", shape=box, style=filled, fillcolor=lightyellow];
  updated_by -> RFC7364;

  cites [label="cites", shape=box, style=filled, fillcolor=lightblue];
  RFC7364 -> cites;
  RFC7365 [label="RFC7365
Framework for Data Center (DC) Network
Virtualization
2014", shape=ellipse, URL="RFC7365.html", target="_top", tooltip="This document provides a framework for Data Center (DC) Network Virtualization over Layer 3 (NVO3) and defines a reference model along with logical components required to design a solution."];
  cites -> RFC7365;
  RFC4364 [label="RFC4364
BGP/MPLS IP Virtual Private Networks
(VPNs)
2006", shape=ellipse, URL="RFC4364.html", target="_top", tooltip="This document describes a method by which a Service Provider may use an IP backbone to provide IP Virtual Private Networks (VPNs) for its customers.  This method uses a peer model, in which the customers' edge routers (CE routers) send their routes to the Service Provider's edge routers (PE routers); there is no overlay visible to the customer's routing algorithm, and CE routers at different sites do not peer with each other.  Data packets are tunneled through the backbone, so that the core routers do not need to know the VPN routes. [STANDARDS-TRACK]"];
  cites -> RFC4364;
  RFC6325 [label="RFC6325
Routing Bridges (RBridges): Base
Protocol Specification
2011", shape=ellipse, URL="RFC6325.html", target="_top", tooltip="Routing Bridges (RBridges) provide optimal pair-wise forwarding without configuration, safe forwarding even during periods of temporary loops, and support for multipathing of both unicast and multicast traffic. They achieve these goals using IS-IS routing and encapsulation of traffic with a header that includes a hop count. RBridges are compatible with previous IEEE 802.1 customer bridges as well as IPv4 and IPv6 routers and end nodes. They are as invisible to current IP routers as bridges are and, like routers, they terminate the bridge spanning tree protocol. The design supports VLANs and the optimization of the distribution of multi-destination frames based on VLAN ID and based on IP-derived multicast groups. It also allows unicast forwarding tables at transit RBridges to be sized according to the number of RBridges (rather than the number of end nodes), which allows their forwarding tables to be substantially smaller than in conventional customer bridges. [STANDARDS-TRACK]"];
  cites -> RFC6325;
  RFC6830 [label="RFC6830
The Locator/ID Separation Protocol
(LISP)
2013", shape=ellipse, URL="RFC6830.html", target="_top", tooltip="This document describes a network-layer-based protocol that enables separation of IP addresses into two new numbering spaces: Endpoint Identifiers (EIDs) and Routing Locators (RLOCs). No changes are required to either host protocol stacks or to the core of the Internet infrastructure. The Locator/ID Separation Protocol (LISP) can be incrementally deployed, without a flag day, and offers Traffic Engineering, multihoming, and mobility benefits to early adopters, even when there are relatively few LISP-capable sites. Design and development of LISP was largely motivated by the problem statement produced by the October 2006 IAB Routing and Addressing Workshop. This document defines an Experimental Protocol for the Internet community."];
  cites -> RFC6830;
  RFC6820 [label="RFC6820
Address Resolution Problems in Large
Data Center Networks
2013", shape=ellipse, URL="RFC6820.html", target="_top", tooltip="This document examines address resolution issues related to the scaling of data centers with a very large number of hosts.  The scope of this document is relatively narrow, focusing on address resolution (the Address Resolution Protocol (ARP) in IPv4 and Neighbor Discovery (ND) in IPv6) within a data center.  This document is a product of the Internet Engineering Task Force (IETF)."];
  cites -> RFC6820;
  RFC7172 [label="RFC7172
Transparent Interconnection of Lots of
Links (TRILL): Fine-Grained Labeling
2014", shape=ellipse, URL="RFC7172.html", target="_top", tooltip="The IETF has standardized Transparent Interconnection of Lots of Links (TRILL), a protocol for least-cost transparent frame routing in multi-hop networks with arbitrary topologies and link technologies, using link-state routing and a hop count.  The TRILL base protocol standard supports the labeling of TRILL Data packets with up to 4K IDs.  However, there are applications that require a larger number of labels providing configurable isolation of data.  This document updates RFC 6325 by specifying optional extensions to the TRILL base protocol to safely accomplish this.  These extensions, called fine-grained labeling, are primarily intended for use in large data centers, that is, those with more than 4K users requiring configurable data isolation from each other."];
  cites -> RFC7172;
  RFC3931 [label="RFC3931
Layer Two Tunneling Protocol - Version 3
(L2TPv3)
2005", shape=ellipse, URL="RFC3931.html", target="_top", tooltip="This document describes version 3 of the Layer Two Tunneling Protocol (L2TPv3).  L2TPv3 defines the base control protocol and encapsulation for tunneling multiple Layer 2 connections between two IP nodes.  Additional documents detail the specifics for each data link type being emulated. [STANDARDS-TRACK]"];
  cites -> RFC3931;
  RFC5213 [label="RFC5213
Proxy Mobile IPv6
2008", shape=ellipse, URL="RFC5213.html", target="_top", tooltip="Network-based mobility management enables IP mobility for a host without requiring its participation in any mobility-related signaling.  The network is responsible for managing IP mobility on behalf of the host.  The mobility entities in the network are responsible for tracking the movements of the host and initiating the required mobility signaling on its behalf.  This specification describes a network-based mobility management protocol and is referred to as Proxy Mobile IPv6. [STANDARDS-TRACK]"];
  cites -> RFC5213;
  RFC5844 [label="RFC5844
IPv4 Support for Proxy Mobile IPv6
2010", shape=ellipse, URL="RFC5844.html", target="_top", tooltip="This document specifies extensions to the Proxy Mobile IPv6 protocol for adding IPv4 protocol support.  The scope of IPv4 protocol support is two-fold: 1) enable IPv4 home address mobility support to the mobile node, and 2) allow the mobility entities in the Proxy Mobile IPv6 domain to exchange signaling messages over an IPv4 transport network. [STANDARDS-TRACK]"];
  cites -> RFC5844;
  RFC5845 [label="RFC5845
Generic Routing Encapsulation (GRE) Key
Option for Proxy Mobile IPv6
2010", shape=ellipse, URL="RFC5845.html", target="_top", tooltip="This specification defines a new mobility option for allowing the mobile access gateway and the local mobility anchor to negotiate Generic Routing Encapsulation (GRE) encapsulation mode and exchange the downlink and uplink GRE keys that are used for marking the downlink and uplink traffic that belong to a specific mobility session.  In addition, the same mobility option can be used to negotiate the GRE encapsulation mode without exchanging the GRE keys. [STANDARDS-TRACK]"];
  cites -> RFC5845;
  RFC6245 [label="RFC6245
Generic Routing Encapsulation (GRE) Key
Extension for Mobile IPv4
2011", shape=ellipse, URL="RFC6245.html", target="_top", tooltip="The Generic Routing Encapsulation (GRE) specification contains a Key field, which MAY contain a value that is used to identify a particular GRE data stream.  This specification defines a new Mobile IP extension that is used to exchange the value to be used in the GRE Key field.  This extension further allows the Mobility Agents to set up the necessary protocol interfaces prior to receiving the mobile node traffic.  The new extension allows a Foreign Agent to request GRE tunneling without disturbing the Home Agent behavior specified for Mobile IPv4.  GRE tunneling with the Key field allows the operators to have home networks that consist of multiple Virtual Private Networks (VPNs), which may have overlapping home addresses.  When the tuple <Care of Address, Home Address, and Home Agent Address> is the same across multiple subscriber sessions, GRE tunneling will provide a means for the Foreign Agent and Home Agent to identify data streams for the individual sessions based on the GRE key.  In the absence of this key identifier, the data streams cannot be distinguished from each other -- a significant drawback when using IP-in-IP tunneling. [STANDARDS-TRACK]"];
  cites -> RFC6245;

  rev_cites [label="cites", shape=box, style=filled, fillcolor=lightblue];
  rev_cites -> RFC7364;
  RFC7365 [label="RFC7365
Framework for Data Center (DC) Network
Virtualization
2014", shape=ellipse, URL="RFC7365.html", target="_top", tooltip="This document provides a framework for Data Center (DC) Network Virtualization over Layer 3 (NVO3) and defines a reference model along with logical components required to design a solution."];
  RFC7365 -> rev_cites;
  RFC7586 [label="RFC7586
The Scalable Address Resolution Protocol
(SARP) for Large Data Centers
2015", shape=ellipse, URL="RFC7586.html", target="_top", tooltip="This document introduces the Scalable Address Resolution Protocol (SARP), an architecture that uses proxy gateways to scale large data center networks.  SARP is based on fast proxies that significantly reduce switches' Filtering Database (FDB) table sizes and reduce impact of ARP and Neighbor Discovery (ND) on network elements in an environment where hosts within one subnet (or VLAN) can spread over various locations.  SARP is targeted for massive data centers with a significant number of Virtual Machines (VMs) that can move across various physical locations."];
  RFC7586 -> rev_cites;
  RFC8014 [label="RFC8014
An Architecture for Data-Center Network
Virtualization over Layer 3 (NVO3)
2016", shape=ellipse, URL="RFC8014.html", target="_top", tooltip="This document presents a high-level overview architecture for building data-center Network Virtualization over Layer 3 (NVO3) networks.  The architecture is given at a high level, showing the major components of an overall system.  An important goal is to divide the space into individual smaller components that can be implemented independently with clear inter-component interfaces and interactions.  It should be possible to build and implement individual components in isolation and have them interoperate with other independently implemented components.  That way, implementers have flexibility in implementing individual components and can optimize and innovate within their respective components without requiring changes to other components."];
  RFC8014 -> rev_cites;
  RFC8151 [label="RFC8151
Use Cases for Data Center Network
Virtualization Overlay Networks
2017", shape=ellipse, URL="RFC8151.html", target="_top", tooltip="This document describes Network Virtualization over Layer 3 (NVO3) use cases that can be deployed in various data centers and serve different data-center applications."];
  RFC8151 -> rev_cites;
  RFC8293 [label="RFC8293
A Framework for Multicast in Network
Virtualization over Layer 3
2018", shape=ellipse, URL="RFC8293.html", target="_top", tooltip="This document provides a framework for supporting multicast traffic in a network that uses Network Virtualization over Layer 3 (NVO3).  Both infrastructure multicast and application-specific multicast are discussed.  It describes the various mechanisms that can be used for delivering such traffic as well as the data plane and control plane considerations for each of the mechanisms."];
  RFC8293 -> rev_cites;
  RFC8365 [label="RFC8365
A Network Virtualization Overlay
Solution Using Ethernet VPN (EVPN)
2018", shape=ellipse, URL="RFC8365.html", target="_top", tooltip="This document specifies how Ethernet VPN (EVPN) can be used as a Network Virtualization Overlay (NVO) solution and explores the various tunnel encapsulation options over IP and their impact on the EVPN control plane and procedures.  In particular, the following encapsulation options are analyzed: Virtual Extensible LAN (VXLAN), Network Virtualization using Generic Routing Encapsulation (NVGRE), and MPLS over GRE.  This specification is also applicable to Generic Network Virtualization Encapsulation (GENEVE); however, some incremental work is required, which will be covered in a separate document.  This document also specifies new multihoming procedures for split-horizon filtering and mass withdrawal.  It also specifies EVPN route constructions for VXLAN/NVGRE encapsulations and Autonomous System Border Router (ASBR) procedures for multihoming of Network Virtualization Edge (NVE) devices."];
  RFC8365 -> rev_cites;
  RFC8394 [label="RFC8394
Split Network Virtualization Edge
(Split-NVE) Control-Plane Requirements
2018", shape=ellipse, URL="RFC8394.html", target="_top", tooltip="In the Split Network Virtualization Edge (Split-NVE) architecture, the functions of the NVE are split across a server and a piece of external network equipment that is called an External NVE. The server-resident control-plane functionality resides in control software, which may be part of hypervisor or container-management software; for simplicity, this document refers to the hypervisor as the location of this software. One or more control-plane protocols between a hypervisor and its associated External NVE(s) are used by the hypervisor to distribute its virtual-machine networking state to the External NVE(s) for further handling. This document illustrates the functionality required by this type of control-plane signaling protocol and outlines the high-level requirements. Virtual-machine states as well as state transitioning are summarized to help clarify the protocol requirements."];
  RFC8394 -> rev_cites;
  RFC9469 [label="RFC9469
Applicability of Ethernet Virtual
Private Network (EVPN) to Network
Virtualization over Layer 3 (NVO3)
Networks
2023", shape=ellipse, URL="RFC9469.html", target="_top", tooltip="An Ethernet Virtual Private Network (EVPN) provides a unified control plane that solves the issues of Network Virtualization Edge (NVE) auto-discovery, tenant Media Access Control (MAC) / IP dissemination, and advanced features in a scablable way as required by Network Virtualization over Layer 3 (NVO3) networks.  EVPN is a scalable solution for NVO3 networks and keeps the independence of the underlay IP Fabric, i.e., there is no need to enable Protocol Independent Multicast (PIM) in the underlay network and maintain multicast states for tenant Broadcast Domains.  This document describes the use of EVPN for NVO3 networks and discusses its applicability to basic Layer 2 and Layer 3 connectivity requirements and to advanced features such as MAC Mobility, MAC Protection and Loop Protection, multihoming, Data Center Interconnect (DCI), and much more.  No new EVPN procedures are introduced."];
  RFC9469 -> rev_cites;
}
