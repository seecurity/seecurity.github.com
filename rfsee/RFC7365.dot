digraph Flow {
  rankdir=LR;
  splines=true;
  concentrate=true;   // merge parallel edges / reduce clutter
  nodesep=0.25;       // horizontal/vertical separation between nodes
  ranksep=0.35;       // separation between ranks (columns in LR)
  RFC7365 [label="RFC7365
Framework for Data Center (DC) Network
Virtualization
2014", shape=ellipse, fillcolor="#e8f0ff", tooltip="This document provides a framework for Data Center (DC) Network Virtualization over Layer 3 (NVO3) and defines a reference model along with logical components required to design a solution.", style=filled, fillcolor=green]; 

  obs_by [label="obsoletes", shape=box, style=filled, fillcolor=lightblue];
  obs_by -> RFC7365;

  obs [label="obsoletes", shape=box, style=filled, fillcolor=lightblue];
  RFC7365 -> obs;

  updates [label="updates", shape=box, style=filled, fillcolor=lightyellow];
  RFC7365 -> updates;

  updated_by [label="updates", shape=box, style=filled, fillcolor=lightyellow];
  updated_by -> RFC7365;

  cites [label="cites", shape=box, style=filled, fillcolor=lightblue];
  RFC7365 -> cites;
  RFC7364 [label="RFC7364
Problem Statement: Overlays for Network
Virtualization
2014", shape=ellipse, URL="RFC7364.html", target="_top", tooltip="This document describes issues associated with providing multi-tenancy in large data center networks and how these issues may be addressed using an overlay-based network virtualization approach.  A key multi-tenancy requirement is traffic isolation so that one tenant's traffic is not visible to any other tenant.  Another requirement is address space isolation so that different tenants can use the same address space within different virtual networks.  Traffic and address space isolation is achieved by assigning one or more virtual networks to each tenant, where traffic within a virtual network can only cross into another virtual network in a controlled fashion (e.g., via a configured router and/or a security gateway).  Additional functionality is required to provision virtual networks, associating a virtual machine's network interface(s) with the appropriate virtual network and maintaining that association as the virtual machine is activated, migrated, and/or deactivated.  Use of an overlay-based approach enables scalable deployment on large network infrastructures."];
  cites -> RFC7364;
  RFC4761 [label="RFC4761
Virtual Private LAN Service (VPLS) Using
BGP for Auto-Discovery and Signaling
2007", shape=ellipse, URL="RFC4761.html", target="_top", tooltip="Virtual Private LAN Service (VPLS), also known as Transparent LAN Service and Virtual Private Switched Network service, is a useful Service Provider offering. The service offers a Layer 2 Virtual Private Network (VPN); however, in the case of VPLS, the customers in the VPN are connected by a multipoint Ethernet LAN, in contrast to the usual Layer 2 VPNs, which are point-to-point in nature. This document describes the functions required to offer VPLS, a mechanism for signaling a VPLS, and rules for forwarding VPLS frames across a packet switched network. [STANDARDS-TRACK]"];
  cites -> RFC4761;
  RFC4762 [label="RFC4762
Virtual Private LAN Service (VPLS) Using
Label Distribution Protocol (LDP)
Signaling
2007", shape=ellipse, URL="RFC4762.html", target="_top", tooltip="This document describes a Virtual Private LAN Service (VPLS) solution using pseudowires, a service previously implemented over other tunneling technologies and known as Transparent LAN Services (TLS). A VPLS creates an emulated LAN segment for a given set of users; i.e., it creates a Layer 2 broadcast domain that is fully capable of learning and forwarding on Ethernet MAC addresses and that is closed to a given set of users. Multiple VPLS services can be supported from a single Provider Edge (PE) node. This document describes the control plane functions of signaling pseudowire labels using Label Distribution Protocol (LDP), extending RFC 4447. It is agnostic to discovery protocols. The data plane functions of forwarding are also described, focusing in particular on the learning of MAC addresses. The encapsulation of VPLS packets is described by RFC 4448. [STANDARDS-TRACK]"];
  cites -> RFC4762;
  RFC4364 [label="RFC4364
BGP/MPLS IP Virtual Private Networks
(VPNs)
2006", shape=ellipse, URL="RFC4364.html", target="_top", tooltip="This document describes a method by which a Service Provider may use an IP backbone to provide IP Virtual Private Networks (VPNs) for its customers.  This method uses a peer model, in which the customers' edge routers (CE routers) send their routes to the Service Provider's edge routers (PE routers); there is no overlay visible to the customer's routing algorithm, and CE routers at different sites do not peer with each other.  Data packets are tunneled through the backbone, so that the core routers do not need to know the VPN routes. [STANDARDS-TRACK]"];
  cites -> RFC4364;
  RFC6820 [label="RFC6820
Address Resolution Problems in Large
Data Center Networks
2013", shape=ellipse, URL="RFC6820.html", target="_top", tooltip="This document examines address resolution issues related to the scaling of data centers with a very large number of hosts.  The scope of this document is relatively narrow, focusing on address resolution (the Address Resolution Protocol (ARP) in IPv4 and Neighbor Discovery (ND) in IPv6) within a data center.  This document is a product of the Internet Engineering Task Force (IETF)."];
  cites -> RFC6820;
  RFC1191 [label="RFC1191
Path MTU discovery
1990", shape=ellipse, URL="RFC1191.html", target="_top", tooltip="This memo describes a technique for dynamically discovering the maximum transmission unit (MTU) of an arbitrary internet path.  It specifies a small change to the way routers generate one type of ICMP message.  For a path that passes through a router that has not been so changed, this technique might not discover the correct Path MTU, but it will always choose a Path MTU as accurate as, and in many cases more accurate than, the Path MTU that would be chosen by current practice. [STANDARDS-TRACK]"];
  cites -> RFC1191;
  RFC1981 [label="RFC1981
Path MTU Discovery for IP version 6
1996", shape=ellipse, URL="RFC1981.html", target="_top", tooltip="This document describes Path MTU Discovery for IP version 6.  It is largely derived from RFC 1191, which describes Path MTU Discovery for IP version 4. [STANDARDS-TRACK]"];
  cites -> RFC1981;
  RFC4821 [label="RFC4821
Packetization Layer Path MTU Discovery
2007", shape=ellipse, URL="RFC4821.html", target="_top", tooltip="This document describes a robust method for Path MTU Discovery (PMTUD) that relies on TCP or some other Packetization Layer to probe an Internet path with progressively larger packets.  This method is described as an extension to RFC 1191 and RFC 1981, which specify ICMP-based Path MTU Discovery for IP versions 4 and 6, respectively. [STANDARDS-TRACK]"];
  cites -> RFC4821;
  RFC3148 [label="RFC3148
A Framework for Defining Empirical Bulk
Transfer Capacity Metrics
2001", shape=ellipse, URL="RFC3148.html", target="_top", tooltip="This document defines a framework for standardizing multiple BTC (Bulk Transport Capacity) metrics that parallel the permitted transport diversity.  This memo provides information for the Internet community."];
  cites -> RFC3148;
  RFC2679 [label="RFC2679
A One-way Delay Metric for IPPM
1999", shape=ellipse, URL="RFC2679.html", target="_top", tooltip="This memo defines a metric for one-way delay of packets across Internet paths. [STANDARDS-TRACK]"];
  cites -> RFC2679;
  RFC2680 [label="RFC2680
A One-way Packet Loss Metric for IPPM
1999", shape=ellipse, URL="RFC2680.html", target="_top", tooltip="This memo defines a metric for one-way packet loss across Internet paths. [STANDARDS-TRACK]"];
  cites -> RFC2680;
  RFC3393 [label="RFC3393
IP Packet Delay Variation Metric for IP
Performance Metrics (IPPM)
2002", shape=ellipse, URL="RFC3393.html", target="_top", tooltip="None"];
  cites -> RFC3393;

  rev_cites [label="cites", shape=box, style=filled, fillcolor=lightblue];
  rev_cites -> RFC7365;
  RFC7364 [label="RFC7364
Problem Statement: Overlays for Network
Virtualization
2014", shape=ellipse, URL="RFC7364.html", target="_top", tooltip="This document describes issues associated with providing multi-tenancy in large data center networks and how these issues may be addressed using an overlay-based network virtualization approach.  A key multi-tenancy requirement is traffic isolation so that one tenant's traffic is not visible to any other tenant.  Another requirement is address space isolation so that different tenants can use the same address space within different virtual networks.  Traffic and address space isolation is achieved by assigning one or more virtual networks to each tenant, where traffic within a virtual network can only cross into another virtual network in a controlled fashion (e.g., via a configured router and/or a security gateway).  Additional functionality is required to provision virtual networks, associating a virtual machine's network interface(s) with the appropriate virtual network and maintaining that association as the virtual machine is activated, migrated, and/or deactivated.  Use of an overlay-based approach enables scalable deployment on large network infrastructures."];
  RFC7364 -> rev_cites;
  RFC8014 [label="RFC8014
An Architecture for Data-Center Network
Virtualization over Layer 3 (NVO3)
2016", shape=ellipse, URL="RFC8014.html", target="_top", tooltip="This document presents a high-level overview architecture for building data-center Network Virtualization over Layer 3 (NVO3) networks.  The architecture is given at a high level, showing the major components of an overall system.  An important goal is to divide the space into individual smaller components that can be implemented independently with clear inter-component interfaces and interactions.  It should be possible to build and implement individual components in isolation and have them interoperate with other independently implemented components.  That way, implementers have flexibility in implementing individual components and can optimize and innovate within their respective components without requiring changes to other components."];
  RFC8014 -> rev_cites;
  RFC8151 [label="RFC8151
Use Cases for Data Center Network
Virtualization Overlay Networks
2017", shape=ellipse, URL="RFC8151.html", target="_top", tooltip="This document describes Network Virtualization over Layer 3 (NVO3) use cases that can be deployed in various data centers and serve different data-center applications."];
  RFC8151 -> rev_cites;
  RFC8293 [label="RFC8293
A Framework for Multicast in Network
Virtualization over Layer 3
2018", shape=ellipse, URL="RFC8293.html", target="_top", tooltip="This document provides a framework for supporting multicast traffic in a network that uses Network Virtualization over Layer 3 (NVO3).  Both infrastructure multicast and application-specific multicast are discussed.  It describes the various mechanisms that can be used for delivering such traffic as well as the data plane and control plane considerations for each of the mechanisms."];
  RFC8293 -> rev_cites;
  RFC8365 [label="RFC8365
A Network Virtualization Overlay
Solution Using Ethernet VPN (EVPN)
2018", shape=ellipse, URL="RFC8365.html", target="_top", tooltip="This document specifies how Ethernet VPN (EVPN) can be used as a Network Virtualization Overlay (NVO) solution and explores the various tunnel encapsulation options over IP and their impact on the EVPN control plane and procedures.  In particular, the following encapsulation options are analyzed: Virtual Extensible LAN (VXLAN), Network Virtualization using Generic Routing Encapsulation (NVGRE), and MPLS over GRE.  This specification is also applicable to Generic Network Virtualization Encapsulation (GENEVE); however, some incremental work is required, which will be covered in a separate document.  This document also specifies new multihoming procedures for split-horizon filtering and mass withdrawal.  It also specifies EVPN route constructions for VXLAN/NVGRE encapsulations and Autonomous System Border Router (ASBR) procedures for multihoming of Network Virtualization Edge (NVE) devices."];
  RFC8365 -> rev_cites;
  RFC8394 [label="RFC8394
Split Network Virtualization Edge
(Split-NVE) Control-Plane Requirements
2018", shape=ellipse, URL="RFC8394.html", target="_top", tooltip="In the Split Network Virtualization Edge (Split-NVE) architecture, the functions of the NVE are split across a server and a piece of external network equipment that is called an External NVE. The server-resident control-plane functionality resides in control software, which may be part of hypervisor or container-management software; for simplicity, this document refers to the hypervisor as the location of this software. One or more control-plane protocols between a hypervisor and its associated External NVE(s) are used by the hypervisor to distribute its virtual-machine networking state to the External NVE(s) for further handling. This document illustrates the functionality required by this type of control-plane signaling protocol and outlines the high-level requirements. Virtual-machine states as well as state transitioning are summarized to help clarify the protocol requirements."];
  RFC8394 -> rev_cites;
  RFC8926 [label="RFC8926
Geneve: Generic Network Virtualization
Encapsulation
2020", shape=ellipse, URL="RFC8926.html", target="_top", tooltip="Network virtualization involves the cooperation of devices with a wide variety of capabilities such as software and hardware tunnel endpoints, transit fabrics, and centralized control clusters.  As a result of their role in tying together different elements of the system, the requirements on tunnels are influenced by all of these components.  Therefore, flexibility is the most important aspect of a tunneling protocol if it is to keep pace with the evolution of technology.  This document describes Geneve, an encapsulation protocol designed to recognize and accommodate these changing capabilities and needs."];
  RFC8926 -> rev_cites;
  RFC9135 [label="RFC9135
Integrated Routing and Bridging in
Ethernet VPN (EVPN)
2021", shape=ellipse, URL="RFC9135.html", target="_top", tooltip="Ethernet VPN (EVPN) provides an extensible and flexible multihoming VPN solution over an MPLS/IP network for intra-subnet connectivity among Tenant Systems and end devices that can be physical or virtual.  However, there are scenarios for which there is a need for a dynamic and efficient inter-subnet connectivity among these Tenant Systems and end devices while maintaining the multihoming capabilities of EVPN.  This document describes an Integrated Routing and Bridging (IRB) solution based on EVPN to address such requirements."];
  RFC9135 -> rev_cites;
  RFC9136 [label="RFC9136
IP Prefix Advertisement in Ethernet VPN
(EVPN)
2021", shape=ellipse, URL="RFC9136.html", target="_top", tooltip="The BGP MPLS-based Ethernet VPN (EVPN) (RFC 7432) mechanism provides a flexible control plane that allows intra-subnet connectivity in an MPLS and/or Network Virtualization Overlay (NVO) (RFC 7365) network.  In some networks, there is also a need for dynamic and efficient inter-subnet connectivity across Tenant Systems and end devices that can be physical or virtual and do not necessarily participate in dynamic routing protocols.  This document defines a new EVPN route type for the advertisement of IP prefixes and explains some use-case examples where this new route type is used."];
  RFC9136 -> rev_cites;
  RFC9469 [label="RFC9469
Applicability of Ethernet Virtual
Private Network (EVPN) to Network
Virtualization over Layer 3 (NVO3)
Networks
2023", shape=ellipse, URL="RFC9469.html", target="_top", tooltip="An Ethernet Virtual Private Network (EVPN) provides a unified control plane that solves the issues of Network Virtualization Edge (NVE) auto-discovery, tenant Media Access Control (MAC) / IP dissemination, and advanced features in a scablable way as required by Network Virtualization over Layer 3 (NVO3) networks.  EVPN is a scalable solution for NVO3 networks and keeps the independence of the underlay IP Fabric, i.e., there is no need to enable Protocol Independent Multicast (PIM) in the underlay network and maintain multicast states for tenant Broadcast Domains.  This document describes the use of EVPN for NVO3 networks and discusses its applicability to basic Layer 2 and Layer 3 connectivity requirements and to advanced features such as MAC Mobility, MAC Protection and Loop Protection, multihoming, Data Center Interconnect (DCI), and much more.  No new EVPN procedures are introduced."];
  RFC9469 -> rev_cites;
  RFC9521 [label="RFC9521
Bidirectional Forwarding Detection (BFD)
for  Generic Network Virtualization
Encapsulation (Geneve)
2024", shape=ellipse, URL="RFC9521.html", target="_top", tooltip="This document describes the use of the Bidirectional Forwarding Detection (BFD) protocol in point-to-point Generic Network Virtualization Encapsulation (Geneve) unicast tunnels used to make up an overlay network."];
  RFC9521 -> rev_cites;
  RFC9772 [label="RFC9772
Active Operations, Administration, and
Maintenance (OAM) for Use in Generic
Network Virtualization Encapsulation
(Geneve)
2025", shape=ellipse, URL="RFC9772.html", target="_top", tooltip="Geneve (Generic Network Virtualization Encapsulation) is a flexible and extensible network virtualization overlay protocol designed to encapsulate network packets for transport across underlying physical networks.  This document specifies the requirements and provides a framework for Operations, Administration, and Maintenance (OAM) in Geneve networks.  It outlines the OAM functions necessary to monitor, diagnose, and troubleshoot Geneve overlay networks to ensure proper operation and performance.  The document aims to guide the implementation of OAM mechanisms within the Geneve protocol to support network operators in maintaining reliable and efficient virtualized network environments."];
  RFC9772 -> rev_cites;
}
